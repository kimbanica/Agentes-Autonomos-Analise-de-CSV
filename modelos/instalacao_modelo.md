# ğŸ§  InstalaÃ§Ã£o do Modelo `mixtral-8x7b-instruct-v0.1.Q5_K_M.gguf`

Este guia ensina como baixar o modelo **Mixtral 8x7B Instruct (Q5_K_M)** no formato `.gguf` e integrÃ¡-lo ao seu projeto de agente autÃ´nomo.

---

## âœ… PrÃ©-requisitos

- ConexÃ£o com a internet
- 20 GB ou mais de espaÃ§o em disco
- Python 3.10+
- Projeto com pasta `modelos/`

---

## ğŸ”— Link oficial

O modelo pode ser baixado a partir do Hugging Face:

ğŸ‘‰ [https://huggingface.co/TheBloke/Mixtral-8x7B-Instruct-GGUF](https://huggingface.co/TheBloke/Mixtral-8x7B-Instruct-GGUF)

---

## ğŸ“¥ Etapas para baixar

1. Acesse o link acima.
2. Localize o arquivo chamado:

```
mixtral-8x7b-instruct-v0.1.Q5_K_M.gguf
```

3. Clique no nome do arquivo para baixar.
4. ApÃ³s o download, mova o arquivo para dentro da pasta `modelos/` do seu projeto:

```
agente_csv/
â””â”€â”€ modelos/
    â””â”€â”€ mixtral-8x7b-instruct-v0.1.Q5_K_M.gguf
```

---

## âš™ï¸ Exemplo de uso no cÃ³digo

```python
from langchain_community.llms import LlamaCpp

llm = LlamaCpp(
    model_path="./modelos/mixtral-8x7b-instruct-v0.1.Q5_K_M.gguf",
    temperature=0.7,
    max_tokens=512,
    n_ctx=32768,
    verbose=True
)

resposta = llm("Qual a capital do Brasil?")
print(resposta)
```

---

## âš ï¸ ObservaÃ§Ãµes

- **Limite de contexto**: Embora o Mixtral suporte atÃ© 32.000 tokens, isso depende da sua mÃ¡quina. Se ocorrer erro de memÃ³ria ou tokens, tente reduzir `n_ctx` para 4096 ou 2048.
- **Velocidade**: A quantizaÃ§Ã£o Q5_K_M oferece bom equilÃ­brio entre desempenho e precisÃ£o.

---

## ğŸ›‘ Dica de seguranÃ§a

> Evite versionar esse arquivo `.gguf` no GitHub, pois ele Ã© grande. Mantenha fora do repositÃ³rio e oriente os membros da equipe a baixarem manualmente conforme instruÃ§Ãµes acima.

---

ğŸ“ **Dica:** salve este arquivo como `docs/instalacao_modelo.md` no seu projeto.
