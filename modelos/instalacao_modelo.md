# üß† Instala√ß√£o do Modelo `mistral-7b-instruct-v0.1.Q5_K_M.gguf`

Este guia ensina como baixar o modelo **Mistral 7B Instruct (Q5_K_M)** no formato `.gguf` e integr√°-lo ao seu projeto de agente aut√¥nomo.

---

## ‚úÖ Pr√©-requisitos

- Conex√£o com a internet
- 8 GB ou mais de espa√ßo em disco
- Python 3.10+
- Projeto com pasta `modelos/`

---

## üîó Link oficial

O modelo pode ser baixado a partir do Hugging Face:

üëâ [https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.1-GGUF](https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.1-GGUF)

---

## üì• Etapas para baixar

1. Acesse o link acima.
2. Localize o arquivo chamado:

```
mistral-7b-instruct-v0.1.Q5_K_M.gguf
```

3. Clique no nome do arquivo para iniciar o download.
4. Ap√≥s o download, mova o arquivo para dentro da pasta `modelos/` do seu projeto:

```
agente_csv/
‚îî‚îÄ‚îÄ modelos/
    ‚îî‚îÄ‚îÄ mistral-7b-instruct-v0.1.Q5_K_M.gguf
```

---

## ‚öôÔ∏è Exemplo de uso no c√≥digo

```python
from langchain_community.llms import LlamaCpp

llm = LlamaCpp(
    model_path="./modelos/mistral-7b-instruct-v0.1.Q5_K_M.gguf",
    temperature=0,
    max_tokens=512,
    n_ctx=2048,
    verbose=True
)

resposta = llm("Qual a capital do Brasil?")
print(resposta)
```

---

## ‚ö†Ô∏è Observa√ß√µes

- **Limite de contexto**: Este modelo suporta at√© **2048 tokens** no total (prompt + resposta). Perguntas longas ou arquivos CSV extensos podem exceder esse limite e causar erros.
- **Solu√ß√£o**: Reduza o n√∫mero de linhas analisadas ou simplifique a pergunta para evitar exceder `n_ctx`.

---

## üõë Dica de seguran√ßa

> Evite versionar esse arquivo `.gguf` no GitHub, pois ele √© muito grande. Mantenha-o fora do reposit√≥rio (`.gitignore`) e oriente os colegas a baixarem manualmente conforme instru√ß√µes acima.

---

